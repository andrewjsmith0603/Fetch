Instructions to run program:
docker pull ghcr.io/andrewjsmith0603/fetch/fetch_assignment:tag

Task 1 Commentary:
In designing the sentence transformer, there were a few choices made outside of the base level BERT architecture that I implemented. Since the task was to create a sentence transformer model, a pooling technique needs to be used to aggregate the embeddings. For this model, I chose to use a max pooling method as it is a simple and effective method that was well suited for the downstream tasks. Max pooling is useful for the downstream classification tasks as it captures the most significant features from the token embeddings, ensuring that the strongest signals influence the have the most influence on the final decision. This helps highlight important keywords or tokens that are crucial for distinguishing between classes. I also chose to include a member variable for the embedding size so it would be easily accessible in the subsequent multitask model.


Task 2 Commentary:
In order to support multitask learning, the original sentence transformer model was modified with two different linear layers as task heads for sentence classification and sentiment analysis. I just used simple linear layers here since the two different tasks this model aims to complete are very similar, therefore much of the learning can be shared in the higher level layers of the transformer model rather than complex model heads. This architecture could obviously be adapted to have more complex nodes for each head depending on the types of classification and the training and validation data.  I chose to use the cross-entropy loss function for both tasks because it is widely used in classification problems, as it directly compares the predicted probabilities to the true class labels and more effectively penalizes incorrect predictions.


Task 3.1:
There are different implications and advantages to freezing distinct portions of a neural network during training. One simple option is freezing the entire network. This will result in a network that will no longer continue to learn since all of the weights can no longer be changed. This network will use only its pre-trained weights to compute features. The most advantageous reasons to freeze a full network are its efficiency and its simplicity. If the pre-trained model has high performance for the tasks it is intended to complete, additional training could lead to overfitting of the training data and poor generalization.

Another possible option is to freeze only the sentence transformer backbone. This would leave the task-specific heads to be trained, adapting the backbone model to these specific tasks. Advantages of this method are its flexibility and security. Freezing the backbone secures the features extracted from the transformer so that training does not overwrite the knowledge that has already been obtained. With this, leaving the model heads unfrozen leaves the network highly adaptable to tasks similar to those the backbone was pre-trained on.

The final option for discussion is to freeze only one of the two task-specific heads. This is most helpful during the training process to preserve the tuned performance of a specific task head. Freezing weights on a task that is tuning more quickly than others maintains the performance on that task while simultaneously freeing up computational resources to be devoted to the other tasks that fall behind. This option can also be applied to the concept of transfer learning, in which the goal is to provide the network with a new challenge and use the already developed knowledge of that network to aid in solving the new task, once again without ruining other performance.

 Task 3.2:
In a multi-task learning scenario like the one I've chosen involving sentence classification and sentiment analysis, transfer learning can be highly beneficial. The first step is to choose a pre-trained model, like BERT, that was trained on large amounts of diverse data for its ability to capture general-purpose language representations. For other tasks involving domain-specific text like legal or medical documents, a special domain-adapted model would be more appropriate. Next is the choice of when to freeze or unfreeze different layers during training. These decisions are dependent upon what information the model must capture and when it should  be capturing it. The lower transformer layers, which encode general linguistic patterns that are transferable across tasks, would remain frozen throughout the training process to preserve pre-trained knowledge and reduce computational cost. The upper transformer layers, along with the task-specific heads, would be initially trainable to allow adaptation to task-specific features. 

As mentioned above, freezing one task-specific head, such as the sentiment analysis head in my model, might be appropriate if that task's performance is already satisfactory. This ensures that the frozen head maintains its performance while the model focuses on adapting the other. Additionally, gradual unfreezing of the upper transformer layers could also be beneficial. If the models validation performance seems to plateau on one or both tasks, unfreezing some of the higher transformer layers would lead to a more specified model with greater overlap in learning between the two heads. Overall, this transfer learning balances efficiency and adaptability by leveraging the pre-trained embeddings and selectively fine-tuning components to meet the specific needs of multiple tasks.

Task 4:
The multi-task training loop is designed to optimize the model for two closely related tasks. The optimizer chosen was AdamW, a widely used variant of Adam, which I selected for its ability to handle large models efficiently and for its built-in weight decay to prevent overfitting. The learning rate is set to 2e-5, a commonly used value for fine-tuning transformer-based models, ensuring steady but stable updates to the model's weights without overshooting the loss minima. The training loop runs for 5 epochs in this example I chose, which would theoretically balance sufficient iterations while avoiding overfitting to the training data. With a larger dataset (real) dataset it would be most advantageous to begin by increasing or decreasing this number of epochs depending on how training looks.

 During each epoch, the model would iterate through the batches of tokenized data, including input IDs, attention masks, and true labels for both tasks. After a forward pass generates the logits for both classification and sentiment predictions, the respective losses are computed and summed to account for the joint optimization of the two tasks. A backward pass updates the model's parameters based on the aggregated loss, ensuring both tasks influence the learning process, and the average training loss is computed at the end of each epoch to monitor performance and convergence over time.
